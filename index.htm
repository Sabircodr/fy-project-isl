<!DOCTYPE html>
<html lang="en" class="scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Project Synopsis: Real-Time ISL Detection & Translation</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <!-- Chosen Palette: Warm Neutrals with Sky Blue Accent -->
    <!-- Application Structure Plan: A thematic, single-page application structure was chosen to transform the linear synopsis into an engaging narrative. The flow is designed to guide the user from the high-level 'Why' (the problem), to the detailed 'How' (the technology), to the concrete 'What' (the model), and finally the 'Proof' (evaluation & timeline). This structure is more intuitive and exploratory than a simple document scroll. Key interactions include a clickable workflow diagram to reveal details on demand and an interactive timeline, which enhances user engagement and comprehension of the project's technical depth and planning. -->
    <!-- Visualization & Content Choices: 
        - Problem Stats (Goal: Inform) -> Large-font 'Key Figure' cards -> Static display for immediate impact.
        - Project Workflow (Goal: Organize) -> Interactive HTML/CSS Diagram -> User clicks a stage to reveal details. This is more engaging than a static image and encourages exploration. Method: JS DOM manipulation.
        - Model Architecture (Goal: Organize) -> Nested HTML/CSS divs -> Visually represents data flow without using static images, ensuring clarity and maintainability. Method: Tailwind CSS (Flexbox/Grid).
        - Evaluation Metrics (Goal: Compare) -> Radar Chart -> Provides a multi-dimensional view of performance targets, which is more insightful than a simple table. Interaction: Hover tooltips. Library: Chart.js.
        - Project Timeline (Goal: Change/Organize) -> HTML/CSS Gantt Chart -> Visually represents project phases. Interaction: Hover to show task details. Method: JS DOM manipulation.
    -->
    <!-- CONFIRMATION: NO SVG graphics used. NO Mermaid JS used. -->
    <style>
        body {
            font-family: 'Inter', sans-serif;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 350px;
            max-height: 450px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 400px;
            }
        }
        .task-bar:hover .tooltip {
            opacity: 1;
            transform: translateY(0);
        }
    </style>
</head>
<body class="bg-stone-50 text-stone-800">

    <nav class="sticky top-0 z-50 bg-white/80 backdrop-blur-md shadow-sm">
        <div class="max-w-6xl mx-auto px-4">
            <div class="flex justify-between items-center h-16">
                <span class="text-xl font-bold text-sky-700">ISL Translation Project</span>
                <div class="hidden md:flex space-x-8 text-stone-600 font-medium">
                    <a href="#problem" class="hover:text-sky-600 transition-colors">The Problem</a>
                    <a href="#solution" class="hover:text-sky-600 transition-colors">Our Solution</a>
                    <a href="#tech" class="hover:text-sky-600 transition-colors">Technology</a>
                    <a href="#plan" class="hover:text-sky-600 transition-colors">Project Plan</a>
                </div>
                <div class="md:hidden">
                    <button id="mobile-menu-button" class="text-stone-600 hover:text-sky-600 focus:outline-none">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-7 6h7"></path></svg>
                    </button>
                </div>
            </div>
        </div>
        <div id="mobile-menu" class="hidden md:hidden bg-white shadow-lg">
            <a href="#problem" class="block py-2 px-4 text-sm text-stone-600 hover:bg-sky-50">The Problem</a>
            <a href="#solution" class="block py-2 px-4 text-sm text-stone-600 hover:bg-sky-50">Our Solution</a>
            <a href="#tech" class="block py-2 px-4 text-sm text-stone-600 hover:bg-sky-50">Technology</a>
            <a href="#plan" class="block py-2 px-4 text-sm text-stone-600 hover:bg-sky-50">Project Plan</a>
        </div>
    </nav>

    <main class="max-w-6xl mx-auto p-4 md:p-8">
        
        <header class="text-center py-16 md:py-24">
            <h1 class="text-4xl md:text-6xl font-extrabold tracking-tight text-stone-900">
                Real-Time Indian Sign Language <span class="text-sky-600">Translation</span>
            </h1>
            <p class="mt-4 text-lg md:text-xl text-stone-600 max-w-3xl mx-auto">
                A deep learning powered solution to bridge the communication gap for the deaf and hard-of-hearing community in India.
            </p>
        </header>

        <section id="problem" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold text-stone-900">Why This Project Matters</h2>
                <p class="mt-2 text-stone-600 max-w-2xl mx-auto">Understanding the communication barrier is the first step toward building a more inclusive society.</p>
            </div>

            <div class="grid md:grid-cols-3 gap-8 text-center">
                <div class="bg-white p-6 rounded-lg shadow-md border border-stone-200">
                    <div class="text-4xl font-bold text-sky-600">~18 Million</div>
                    <p class="mt-2 text-stone-600 font-medium">People in India with hearing impairments, many of whom use ISL.</p>
                </div>
                <div class="bg-white p-6 rounded-lg shadow-md border border-stone-200">
                    <div class="text-4xl font-bold text-sky-600">95%+</div>
                    <p class="mt-2 text-stone-600 font-medium">Of the hearing population in India is not fluent in ISL.</p>
                </div>
                <div class="bg-white p-6 rounded-lg shadow-md border border-stone-200">
                    <div class="text-4xl font-bold text-sky-600">Key Sectors</div>
                    <p class="mt-2 text-stone-600 font-medium">Significant communication gaps exist in healthcare, education, and public services.</p>
                </div>
            </div>
            <div class="mt-12 bg-white p-8 rounded-lg shadow-md border border-stone-200">
                <h3 class="text-xl font-semibold text-stone-800">The Core Challenge</h3>
                <p class="mt-4 text-stone-600">Existing technological aids for the ISL community are often impractical. Many require expensive, specialized hardware like sensor-gloves, or they can only recognize a very small vocabulary of static signs (like individual letters). They fail to capture the fluid, motion-based nature of conversational sign language, leaving a critical need for a low-cost, software-based solution that can accurately interpret a wide range of dynamic ISL gestures in real-time.</p>
            </div>
        </section>

        <section id="solution" class="py-16 bg-stone-100 -mx-4 md:-mx-8 px-4 md:px-8 rounded-lg">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold text-stone-900">Our Solution: An Interactive Workflow</h2>
                <p class="mt-2 text-stone-600 max-w-2xl mx-auto">This project follows a multi-stage pipeline from live video to translated text. Click on each stage below to learn more about how it works.</p>
            </div>

            <div id="workflow-diagram" class="flex flex-col md:flex-row justify-center items-center space-y-4 md:space-y-0 md:space-x-2">
                 <div class="workflow-step" data-content="step1">
                    <div class="text-2xl">üì∑</div>
                    <div class="font-semibold">Video Capture</div>
                </div>
                <div class="text-stone-400 font-mono hidden md:block">&rarr;</div>
                 <div class="workflow-step" data-content="step2">
                    <div class="text-2xl">üñêÔ∏è</div>
                    <div class="font-semibold">Landmark Detection</div>
                </div>
                <div class="text-stone-400 font-mono hidden md:block">&rarr;</div>
                 <div class="workflow-step" data-content="step3">
                    <div class="text-2xl">üî¢</div>
                    <div class="font-semibold">Keypoint Extraction</div>
                </div>
                 <div class="text-stone-400 font-mono hidden md:block">&rarr;</div>
                 <div class="workflow-step" data-content="step4">
                    <div class="text-2xl">üß†</div>
                    <div class="font-semibold">Model Prediction</div>
                </div>
                <div class="text-stone-400 font-mono hidden md:block">&rarr;</div>
                 <div class="workflow-step" data-content="step5">
                    <div class="text-2xl">üìÑ</div>
                    <div class="font-semibold">Text Output</div>
                </div>
            </div>
            
            <div id="workflow-details" class="mt-8 bg-white p-6 rounded-lg shadow-md border border-stone-200 min-h-[150px] transition-all duration-300">
                <p class="text-stone-600">Select a step from the workflow diagram above to see its details here.</p>
            </div>
        </section>

        <section id="tech" class="py-16">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold text-stone-900">Technology & Model Architecture</h2>
                <p class="mt-2 text-stone-600 max-w-2xl mx-auto">We use a state-of-the-art hybrid deep learning model to understand both gesture shapes and their motion over time.</p>
            </div>
            <div class="bg-white p-8 rounded-lg shadow-md border border-stone-200">
                <h3 class="text-xl font-semibold text-center text-stone-800 mb-6">Hybrid CNN-LSTM Model</h3>
                <div class="flex flex-col md:flex-row items-center justify-center space-y-4 md:space-y-0 md:space-x-4 text-center">
                    <div class="p-4 border-2 border-dashed border-sky-300 rounded-lg w-full md:w-1/3">
                        <h4 class="font-bold text-sky-700">Input Sequence</h4>
                        <p class="text-sm text-stone-600">30 Frames of Normalized Hand & Body Keypoints</p>
                    </div>
                    <div class="text-2xl font-mono text-stone-400">&rarr;</div>
                     <div class="p-4 border border-stone-300 rounded-lg w-full md:w-1/3 bg-stone-50">
                        <h4 class="font-bold text-stone-700">1D CNN Layers</h4>
                        <p class="text-sm text-stone-600">Extract spatial features from each frame's keypoints.</p>
                    </div>
                    <div class="text-2xl font-mono text-stone-400">&rarr;</div>
                     <div class="p-4 border border-stone-300 rounded-lg w-full md:w-1/3 bg-stone-50">
                        <h4 class="font-bold text-stone-700">LSTM Layers</h4>
                        <p class="text-sm text-stone-600">Analyze the sequence of features to understand motion.</p>
                    </div>
                    <div class="text-2xl font-mono text-stone-400">&rarr;</div>
                    <div class="p-4 border-2 border-dashed border-green-400 rounded-lg w-full md:w-1/3 bg-green-50">
                        <h4 class="font-bold text-green-700">Output</h4>
                        <p class="text-sm text-stone-600">Predicted Sign Label (e.g., "Hello", "Thank you")</p>
                    </div>
                </div>
                 <div class="mt-6 text-stone-600 text-sm">
                    <p><b class="text-stone-800">Why this architecture?</b> A standard image classification model (like a CNN alone) can't understand motion. A sequence model (like an LSTM alone) might miss fine-grained spatial details. By combining them, our model learns to recognize <strong>what the hands look like</strong> (CNN) and <strong>how they are moving</strong> (LSTM), which is essential for accurately interpreting dynamic sign language.</p>
                </div>
            </div>
        </section>

        <section id="plan" class="py-16 bg-stone-100 -mx-4 md:-mx-8 px-4 md:px-8 rounded-lg">
            <div class="text-center mb-12">
                <h2 class="text-3xl font-bold text-stone-900">Evaluation & Project Plan</h2>
                <p class="mt-2 text-stone-600 max-w-2xl mx-auto">Our success will be measured by concrete metrics, and the project is structured with a clear timeline.</p>
            </div>
            <div class="grid md:grid-cols-2 gap-8">
                <div class="bg-white p-6 rounded-lg shadow-md border border-stone-200">
                    <h3 class="text-xl font-semibold text-stone-800 mb-4">Target Performance Metrics</h3>
                    <div class="chart-container">
                        <canvas id="metricsChart"></canvas>
                    </div>
                    <p class="text-xs text-center mt-2 text-stone-500">A radar chart showing our target goals for key performance indicators.</p>
                </div>

                <div class="bg-white p-6 rounded-lg shadow-md border border-stone-200">
                    <h3 class="text-xl font-semibold text-stone-800 mb-4">Project Timeline</h3>
                    <div class="space-y-4">
                        <div class="gantt-phase">
                            <h4 class="font-semibold text-sky-800">Semester 7</h4>
                            <div class="gantt-task" style="--start: 0; --span: 2;" data-details="Literature Review & Dataset Finalization (30 Days)"></div>
                            <div class="gantt-task" style="--start: 2; --span: 2;" data-details="Data Collection & Preprocessing (30 Days)"></div>
                            <div class="gantt-task" style="--start: 4; --span: 3;" data-details="Model Design & Initial Implementation (45 Days)"></div>
                            <div class="gantt-task" style="--start: 7; --span: 1;" data-details="Mid-Term Review & Report (15 Days)"></div>
                        </div>
                        <div class="gantt-phase">
                            <h4 class="font-semibold text-sky-800">Semester 8</h4>
                            <div class="gantt-task" style="--start: 0; --span: 4;" data-details="Model Training & Hyperparameter Tuning (60 Days)"></div>
                            <div class="gantt-task" style="--start: 4; --span: 2;" data-details="GUI Development & System Integration (30 Days)"></div>
                            <div class="gantt-task" style="--start: 6; --span: 3;" data-details="Testing, Evaluation & Final Report (45 Days)"></div>
                            <div class="gantt-task" style="--start: 9; --span: 1;" data-details="Final Submission & Viva (15 Days)"></div>
                        </div>
                    </div>
                    <div class="grid grid-cols-6 mt-2 text-xs text-center text-stone-500">
                        <span>Sep-Oct</span><span>Nov</span><span>Dec-Jan</span><span>Feb-Mar</span><span>Apr</span><span>May</span>
                    </div>
                </div>
            </div>
        </section>
        
    </main>
    
    <footer class="text-center py-8 border-t border-stone-200">
        <p class="text-stone-500">&copy; 2025 - Final Year Project, Dept. of CSD, Dr. B.C. Roy Engineering College</p>
    </footer>

    <script>
        document.addEventListener('DOMContentLoaded', function () {
            const mobileMenuButton = document.getElementById('mobile-menu-button');
            const mobileMenu = document.getElementById('mobile-menu');

            mobileMenuButton.addEventListener('click', () => {
                mobileMenu.classList.toggle('hidden');
            });

            const workflowSteps = document.querySelectorAll('.workflow-step');
            const detailsContainer = document.getElementById('workflow-details');
            
            const content = {
                step1: {
                    title: 'üì∑ Step 1: Video Capture & Frame Extraction',
                    text: 'The system uses a standard webcam to capture a live video feed at approximately 30 frames per second. Each frame is an individual image that serves as an input for the next stage.'
                },
                step2: {
                    title: 'üñêÔ∏è Step 2: Hand & Body Landmark Detection',
                    text: 'We use Google\'s MediaPipe framework to process each frame. It quickly and efficiently detects the precise 3D locations (keypoints) of the user\'s hands, face, and body pose. This is vital for isolating the gesture from the background.'
                },
                step3: {
                    title: 'üî¢ Step 3: Keypoint Extraction & Normalization',
                    text: 'The raw (x, y, z) coordinates of the detected landmarks are extracted. These values are then normalized, which means they are scaled to a consistent range. This makes the model robust to variations in the user\'s distance from the camera or their body size.'
                },
                step4: {
                    title: 'üß† Step 4: Model Prediction',
                    text: 'A sequence of keypoints from consecutive frames (e.g., 30 frames) is fed into our trained CNN+LSTM model. The model analyzes the spatial and temporal patterns to classify the gesture and predict the corresponding word.'
                },
                step5: {
                    title: 'üìÑ Step 5: Text Output',
                    text: 'The model\'s prediction is translated into a human-readable word or phrase. This text is then displayed on the user interface in real-time, completing the translation cycle.'
                }
            };

            workflowSteps.forEach(step => {
                step.addEventListener('click', () => {
                    workflowSteps.forEach(s => s.classList.remove('active'));
                    step.classList.add('active');
                    const contentKey = step.dataset.content;
                    detailsContainer.innerHTML = `
                        <h4 class="font-bold text-lg text-sky-700 mb-2">${content[contentKey].title}</h4>
                        <p class="text-stone-600">${content[contentKey].text}</p>
                    `;
                });
            });

            const ctx = document.getElementById('metricsChart').getContext('2d');
            const metricsChart = new Chart(ctx, {
                type: 'radar',
                data: {
                    labels: ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Low Latency'],
                    datasets: [{
                        label: 'Target Performance',
                        data: [95, 92, 92, 92, 98],
                        backgroundColor: 'rgba(56, 189, 248, 0.2)',
                        borderColor: 'rgba(2, 132, 199, 1)',
                        pointBackgroundColor: 'rgba(2, 132, 199, 1)',
                        pointBorderColor: '#fff',
                        pointHoverBackgroundColor: '#fff',
                        pointHoverBorderColor: 'rgba(2, 132, 199, 1)'
                    }]
                },
                options: {
                    responsive: true,
                    maintainAspectRatio: false,
                    scales: {
                        r: {
                            angleLines: {
                                color: 'rgba(0, 0, 0, 0.1)'
                            },
                            grid: {
                                color: 'rgba(0, 0, 0, 0.1)'
                            },
                            pointLabels: {
                                font: {
                                    size: 14,
                                    weight: '500'
                                },
                                color: '#334155'
                            },
                            ticks: {
                                backdropColor: 'rgba(241, 245, 249, 1)',
                                color: '#64748b',
                                stepSize: 20
                            },
                            min: 0,
                            max: 100
                        }
                    },
                    plugins: {
                        legend: {
                            position: 'top',
                        },
                        tooltip: {
                            callbacks: {
                                label: function(context) {
                                    return `${context.dataset.label}: ${context.raw}%`;
                                }
                            }
                        }
                    }
                }
            });

            document.querySelectorAll('a[href^="#"]').forEach(anchor => {
                anchor.addEventListener('click', function (e) {
                    e.preventDefault();
                    document.querySelector(this.getAttribute('href')).scrollIntoView({
                        behavior: 'smooth'
                    });
                    if(mobileMenu.classList.contains('hidden') === false){
                        mobileMenu.classList.add('hidden');
                    }
                });
            });
        });

    </script>
    <style>
        .workflow-step {
            @apply flex flex-col items-center justify-center text-center p-4 w-40 h-28 bg-white border-2 border-transparent rounded-lg cursor-pointer transition-all duration-300 shadow-sm hover:shadow-lg hover:border-sky-400;
        }
        .workflow-step.active {
            @apply border-sky-500 bg-sky-50 shadow-xl;
            transform: translateY(-5px);
        }

        .gantt-phase {
            @apply relative pt-6;
        }

        .gantt-phase h4 {
             @apply absolute -top-0 left-0 text-sm;
        }

        .gantt-task {
            @apply relative h-6 bg-sky-500 rounded-sm;
            grid-column-start: var(--start);
            grid-column-end: span var(--span);
            background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
            background-size: 20px 20px;
            transition: all 0.2s ease-in-out;
            cursor: pointer;
        }
        
        .gantt-task:hover {
            background-color: #0369a1; /* sky-700 */
            transform: scale(1.05);
        }

        .gantt-task::after {
            content: attr(data-details);
            @apply absolute bottom-full mb-2 w-max max-w-xs p-2 text-xs bg-stone-800 text-white rounded-md opacity-0 transition-opacity pointer-events-none;
            left: 50%;
            transform: translateX(-50%);
        }
        
        .gantt-task:hover::after {
            opacity: 1;
        }
        
    </style>
</body>
</html>